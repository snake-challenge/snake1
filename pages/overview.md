# Overview

**Context.** Competitions and challenges are commonly used both in the machine learning community – for boosting the design and development of practical and efficient solutions to hard or new problems – and in the security community – for training purposes or for the evaluation of existing infrastructures. In contrast, in the privacy community, there is not a long tradition of holding such challenges. 

However, in recent years several competitions focusing on data sanitization algorithms (also called data anonymization algorithms or privacy- preserving data publishing algorithms) have been launched. Some of them, like the 2018 Differential Privacy NIST Challenge, have focused primarily on the defense aspect. Others, like the Hide-and-Seek challenge, the INSAnonym competition or the series of PWSCUP competition additionally consider attacks on the sanitized datasets generated by the participants. Finally, more recently, the MICO challenge has targeted the development of white-box membership inference attacks against differentially-private classification models.

**The SNAKE (SaNitization Algorithm under attacK ...ε) challenge focuses specifically on membership inference attacks on differentially-private synthetic data generation algorithms.** The challenge is organized as part of APVP (https://apvp23.sciencesconf.org), the French workshop on the protection of privacy but is open to anyone. In addition, an online session will be organized during APVP to discuss the results of the competition.

![flow](https://snake-challenge.github.io/flow.png)

**Sanitization algorithms under attacks.** The sanitization algorithms under attack during SNAKE are differentially-private synthetic data generation algorithms. More precisely, we have selected a set of algorithms according to the following two criteria: 
technical soundness assessed by a rigorous peer-selection process (*e.g.*, published at top-tier conferences or winner of a dedicated competition) and available open- source implementation. In particular, we have used the implementation available in the reprosyn package (https://github.com/alan-turing-institute/reprosyn). Except for parameters related to differential privacy, we use the default values set in their implementations. More precisely, the sanitization algorithms under attacks are the following:

*  The **PrivBayes** algorithm, which has been shown to satisfy ε-differential privacy, generates synthetic data by capturing the underlying distribution of the private data through a specific Bayesian network. 
*  The **MST** algorithm is a generalization of the NIST-MST algorithm, which has won the 2018 NIST Differential Privacy Synthetic Data challenge. It generates synthetic data by perturbing the marginals that capture the data distribution through the Gaussian mechanism and by post-processing them through the Private-PGM algorithm. The MST algorithm has been proved to satisfy (ε, δ)-differential privacy.
*  The **PATE-GAN** algorithm is an extension of generative adversarial networks (GAN) based on the private aggregation of teacher ensembles framework (PATE). This method has been shown to satisfy (ε, δ)-differential privacy.

**Input.** We provide to the attack algorithms the following information:
• The synthetic dataset generated by an execution of the targeted sanitization algorithm over a private dataset.
• The base dataset from which the private dataset is sampled.
• The parameters of the execution of the sanitization algorithm attacked.
• The targets for which the attack algorithm has to predict the membership.

You can find the input data as release artifacts at https://github.com/snake-challenge/snake1/

**Output.** The output of the attack algorithm is a single real ∈ [0, 1] indicating the predicted probability of each target being within the private dataset or not.

**Targets and background knowledge.** In SNAKE, any household that contains at least 5 individuals might be a target, with the target consisting of the full set of records of the household (*see the Data section for the detailed description of the datasets*). The set of targets given to a team for launching its membership inference attack on a given sanitization algorithm are extracted from the corresponding private dataset.

SNAKE considers the following background knowledge about each target. The adversary knows (1) the exact records of the household targeted, and (2) the full base dataset. Additionally, following Kerckhoffs’s principle, the adversary is also given the information about the sanitization algorithm targeted as well as the parameters used for the executions and has access to its implementation. However, the randomness generated internally during the execution of the algorithm is unknown to the adversary (*e.g.*, for the generation of the Laplace noise).

**Success measure.** The success of a team is computed by first measuring the successes of its attack on each parameterized algorithm attacked (*e.g.*, MST parameterized by (ε = 1.0, δ = 10−5)) and second by aggregating the success measures in a single final score. More precisely, the success of a given attack for a given triple is evaluated based on the well-known membership advantage measure, which is computed for each attack through the execution of a membership experiment (*see the Evaluation section for more information*).

For more details about the competition, see the detailed description paper at the following address:
See https://snake-challenge.github.io