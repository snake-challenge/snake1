# Overview

**Context.** Competitions and challenges are commonly used both in the machine learning community – for boosting the design and development of practical and efficient solutions to hard or new problems – and in the security community – for training purposes or for the evaluation of existing infrastructures. In contrast, in the privacy community, there is not a long tradition of holding such challenges.

However, in recent years several competitions focusing on data sanitization algorithms (also called data anonymization algorithms or privacy- preserving data publishing algorithms) have been launched. Some of them, like the 2018 Differential Privacy NIST Challenge, have focused primarily on the defense aspect. Others, like the Hide-and-Seek challenge, the INSAnonym competition or the PWSCUP1 additionally consider attacks on the sanitized datasets generated by the participants.

**The SNAKE (SaNitization Algorithm under attacK ...ε) challenge focuses specifically on membership inference attacks on differentially-private synthetic data generation algorithms.**

**Sanitization algorithms under attacks.** The sanitization algorithms under attack during SNAKE are differentially-private synthetic data generation algorithms. More precisely, we have selected a set of algorithms according to the following two criteria: technical soundness assessed by a rigorous peer-selection process (*e.g.*, published at top-tier conferences or winner of a dedicated competition) and available open- source implementation. In particular, we have used the implementation available in the reprosyn package (https://github.com/alan-turing-institute/reprosyn). Except for parameters related to differential privacy, we use the default values set in their implementations. More precisely, the sanitization algorithms under attacks are the following:

*  The **PrivBayes** algorithm, which has been shown to satisfy ε-differential privacy, generates synthetic data by capturing the underlying distribution of the private data through a specific Bayesian network. PrivBayes is divided in two main steps, with half of the privacy budget being used for each step. First, it greedily constructs a low-degree Bayesian network by selecting the child/parents pairs maximizing the mutual information based on the Exponential mechanism. Second, it computes the conditional distributions of the resulting network, perturbed by the Laplace mechanism. The synthetic data is finally generated by sampling iteratively from the conditional distributions, which is a form of post- processing preserving the differential privacy guarantees.
*  The **MST** algorithm is a generalization of the NIST-MST algorithm, which has won the 2018 NIST Differential Privacy Synthetic Data challenge. It generates synthetic data by perturbing the marginals that capture the data distribution through the Gaussian mechanism and by post-processing them through the Private-PGM algorithm. The base marginals can be obtained by one of the three following methods: given by a domain expert, computed from a public dataset that follows a data distribution similar to the private dataset’s distribution or computed from the private dataset while satisfying differential privacy. Afterwards, Private-PGM takes as input the perturbed marginals and computes a data distribution that would have produced close marginals. More precisely, it represents the problem as an optimization problem, the distribution searched as a graphical model, and thus solves the problem of finding the optimal graphical model. Synthetic data can then be generated based on the model found. The MST algorithm has been demonstrated to satisfy (ε, δ)-differential privacy.
*  The **PATE-GAN** algorithm is an extension of generative adversarial networks (GAN) based on the private aggregation of teacher ensembles framework (PATE). Each PATE-GAN iteration consists in three phases. First, PATE-GAN trains k classifiers (called teachers) for distinguishing private real data from generated synthetic data. Each teacher is fed with samples from the real dataset (one partition per teacher) and from a generator (uniform distribution at first) and learns to distinguish between the two. Second, PATE-GAN builds on the k teachers for training the student (a binary classifier as well) and the generator in an adversarial manner. The generator produces a set of samples but the teachers are now used for voting, for each sample, whether it is realistic or fake. The number of votes for each label (realistic or fake) is perturbed by the Laplace mechanism and the final label of a sample is the majority label. The student learns to distinguish between real and fake samples based on the resulting dataset. Third, the generator outputs a final set of samples and is penalized according to the success of the student in distinguishing realistic samples from fake samples. PATE-GAN iterates until the privacy budget is exhausted. This method has been shown to satisfy (ε, δ)-differential privacy.

**Input.** We provide to the attack algorithms the following information:
- The synthetic dataset generated by an execution of the targeted sanitization algorithm over a private dataset.
- The base dataset from which the private dataset is sampled.
- The parameters of the execution of the sanitization algorithm attacked.
- The targets for which the attack algorithm has to predict the membership.

You can find the input data as release artifacts at https://github.com/snake-challenge/snake1/

**Output.** The output of the attack algorithm is a single real ∈ [0, 1] indicating the predicted probability of each target being within the private dataset or not.

**Targets and background knowledge.** In SNAKE, any household that contains at least 5 individuals might be a target, with the target consisting of the full set of records of the household (*see the Data section for the detailed description of the datasets*). The set of targets given to a team for launching its membership inference attack on a given sanitization algorithm are extracted from the corresponding private dataset.

SNAKE considers the following background knowledge about each target. The adversary knows (1) the exact records of the household targeted, and (2) the full base dataset. Additionally, following Kerckhoffs’s principle, the adversary is also given the information about the sanitization algorithm targeted as well as the parameters used for the executions and has access to its implementation. However, the randomness generated internally during the execution of the algorithm is unknown to the adversary (*e.g.*, for the generation of the Laplace noise).

**Success measure.** The success of a team is computed by first measuring the successes of its attack on each parameterized algorithm attacked (*e.g.*, MST parameterized by (ε = 1.0, δ = 10−5)) and second by aggregating the success measures in a single final score. More precisely, the success of a given attack for a given triple is evaluated based on the well-known membership advantage measure, which is computed for each attack through the execution of a membership experiment (*see the Evaluation section for more information*).

For more details about the competition, see the detailed description paper at the following address:
See https://snake-challenge.github.io